{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec39dc-3d8a-45c6-8543-a295a45058c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install -U deepchecks[nlp]\n",
    "# !{sys.executable} -m pip install -U deepchecks[nlp-properties]  # Optional - used to compute the more advanced properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf3933e-257f-4aa5-b906-07f662657c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from deepchecks.nlp.text_data import TextData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d6a9d-1e5a-4088-ab65-4fdd7a68b2ae",
   "metadata": {},
   "source": [
    "In this notebook we will go over:\n",
    "1. Creating a TextData object and auto calculating properties\n",
    "2. Running the built-in suites\n",
    "3. Check spotlight - Embeddings drift and Under-Annotated Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033de144-808d-4a18-acd3-a252f4ee4275",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b91fe36-3ca6-46d6-a25f-67ec896bd5b7",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd36041-e294-48f8-bdf5-56e3094b97fa",
   "metadata": {},
   "source": [
    "In this tutorial we will use the tweet emotion dataset, containing tweets and metadata on the users who wrote them. </br>\n",
    "Our goal will be to analyze a model that given a tweet classify its emotion in one of 4 categories: 'happiness', 'anger', 'optimism' and 'sadness'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81787753-4221-4c5c-9536-4993f057417e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.nlp.datasets.classification import tweet_emotion\n",
    "\n",
    "train, test = tweet_emotion.load_data(data_format='DataFrame')\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad50b539-f2a4-41e3-8eb2-77cd2a1bc0c1",
   "metadata": {},
   "source": [
    "We explicitly define our (sorted) classes so that all checks know what classes to expect. This is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dee35b-514b-4c7c-ab42-0a2ec75298e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_classes = ['anger', 'happiness', 'optimism', 'sadness']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a07f60-5afd-48b2-be17-d789ba76df69",
   "metadata": {},
   "source": [
    "## Create TextData Objects (A Deepchecks' Artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87cbdc-76de-4f31-b68d-1208d8818387",
   "metadata": {},
   "source": [
    "Deepchecks' TextData object contain the text samples, labels and possibly also properties and metadata. </br>\n",
    "it stores cache to save time between repeated computations and contain functionalities for input validations and sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405ae0f4-c10e-4af8-887c-2bd2269d10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = TextData(train.text, label=train['label'], task_type='text_classification',\n",
    "                 metadata=train.drop(columns=['label', 'text']))\n",
    "test = TextData(test.text, label=test['label'], task_type='text_classification',\n",
    "                metadata=test.drop(columns=['label', 'text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5dc8a-c635-4261-b55e-82e3945a618c",
   "metadata": {},
   "source": [
    "## Calculating Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58da0ea-b27b-4961-aa4c-f32c5c88241d",
   "metadata": {},
   "source": [
    "Some of Deepchecks' checks uses properties of the text samples for various calculations. </br>\n",
    "Deepcheck have a wide variety of such properties, some simple and some that rely on external models and are more heavy to run. </br>\n",
    "In order for Deepcheck's checks to be able to access the properties they be stored within the TextData object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98e627-468b-4a79-92d9-3a1f0f725722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# properties can be either either calculated directly by Deepchecks or imported for other sources in appropriate format\n",
    "\n",
    "# from torch import device\n",
    "# train.calculate_default_properties(include_long_calculation_properties=True, device=device('mps'))\n",
    "# test.calculate_default_properties(include_long_calculation_properties=True,  device=device('mps'))\n",
    "\n",
    "train.set_properties(pd.read_csv('train_properties.csv'), categorical_properties=['Language'])\n",
    "test.set_properties(pd.read_csv('test_properties.csv'), categorical_properties=['Language'])\n",
    "\n",
    "train.properties.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47331d5-45e5-4440-a0d2-0c058833d096",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Add some missing labesl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f66768-f9ae-4e33-bf2d-2dca6d0b51c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_copy = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c7bd2d-7831-4ad2-9cbc-6654186d0271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "idx_to_fillna = np.random.choice(range(len(test)), int(len(test) * 0.05), replace=False)\n",
    "test_copy._label = test_copy._label.astype(dtype=object)\n",
    "test_copy._label[idx_to_fillna] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b05a07d-2b59-42c4-8117-c00a639407f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "under_unnotated_segment_idx = test_copy.properties[(test_copy.properties.Fluency < 0.4) & (test_copy.properties.Formality < 0.2)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b82131c-ef32-4de0-b49b-352082b14173",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "under_unnotated_segment_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffe0dc-28ac-4ef5-90e2-1c63691584dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "idx_to_fillna = np.random.choice(under_unnotated_segment_idx, int(len(under_unnotated_segment_idx) * 0.4), replace=False)\n",
    "test_copy._label[idx_to_fillna] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0283c81-e746-4ffc-94e0-0af8729750e3",
   "metadata": {},
   "source": [
    "# Running the deepchecks default suites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49bbfa-e8fc-4784-b329-e8dd4c97f981",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Integrity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484efee8-17ce-46aa-a0c7-a2feb672d20d",
   "metadata": {},
   "source": [
    "We will start by doing preliminary integrity check to validate the text formatting. </br>\n",
    "It is recommended to do this step before model training as it may imply additional data engineering is required. </br>\n",
    "\n",
    "We'll do that using the data_integrity pre-built suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fab71b-6626-4a06-af97-26eaf37259ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepchecks.nlp.suites import data_integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473d4db-ba9b-4cd8-9e33-ca8e7aefaab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Increase the default of the Property Outlier check so we can see the results for more than the default 5 properties with the most significant outliers\n",
    "data_integrity_suite = data_integrity()\n",
    "data_integrity_suite.checks[1].n_show_top=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742e12a-6865-4093-8c30-eea2991450e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_integrity_suite.run(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e266ff-d692-421b-a307-fcef140060c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Integrity #1: Unknown Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bed7b19-99ec-493b-bae2-28326d374d3e",
   "metadata": {},
   "source": [
    "First up (in the \"Didn't Pass\" tab) we see that the Unknown Tokens check has returned a problem.\n",
    "\n",
    "Looking at the result, we can see that it assumed (by default) that we're going to use the bert-base-uncased tokenizer for our NLP model, and that if that's the case there are many words in the dataset that contain\n",
    "characters (such as emojies, or Korean characters) that are unrecognized by the tokenizer. This is an important insight, as bert tokenizers are very common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83518968-0320-496a-b67c-d40e72473d80",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Integrity #2: Text outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fba99d9-8b81-48e9-ad47-81fb181fd682",
   "metadata": {},
   "source": [
    "Looking at the Text Outlier check result (in the \"Other\" tab) we can derive several insights: </br>\n",
    "1. hashtags ('#...') are usually several words written together without spaces - we might consider splitting them before feeding the tweet to a model</br>\n",
    "2. In some instances users deliberately misspell words, for example '!' instead of the letter 'l' or 'okayyyyyyyyyy'</br>\n",
    "3. The majority of the data is in English but not all. If we want a classifier that is multi lingual we should collect more data, otherwise we may consider </br>\n",
    "   dropping tweets in other languages from our dataset before training our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f48bda-252d-47fb-92f1-e2e7bc3a50e6",
   "metadata": {},
   "source": [
    "### Integrity #3: Property-Label Correlation (Shortcut Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d397c0-84b0-4172-87c6-df05fca10b99",
   "metadata": {},
   "source": [
    "The Property-Label Correlation check verifies the data does not contain any shortcuts the model can fixate on during the learning process. In our case we can see no indication that this problem exists in our dataset</br> \n",
    "For more information about shortcut learning see: https://towardsdatascience.com/shortcut-learning-how-and-why-models-cheat-1b37575a159"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511f2b92-e794-4b1a-9d3e-d8b0c8e08784",
   "metadata": {},
   "source": [
    "## Train Test Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdca64b-41a3-4042-9450-1bbeb8bda3fb",
   "metadata": {},
   "source": [
    "The next suite serves to validate our split and compare the two dataset. This suite is useful for when a you already decided about your train and test / validation splits, but before training the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc7a31-fbc6-4866-a789-fd4a368870a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepchecks.nlp.suites import train_test_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff78f9-624f-467b-a3d9-a255dff271cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_test_validation().run(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f3015b-89cc-45b0-aa39-5e3b6ae246d3",
   "metadata": {},
   "source": [
    "### Label Drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84572393-b939-4de8-b2c9-74c6ed9cf87c",
   "metadata": {},
   "source": [
    "We can see that we have some significant change in the distribution of the label - the label \"optimism\" is suddenly way more common in the test dataset, while other labels declined. This happened because we split on time, so the topics covered by the tweets in the test dataset may correspond to specific trends or events that happened later in time. Let's investigate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89ec261-6c84-42ba-a183-44bed1251cc0",
   "metadata": {},
   "source": [
    "## Model Evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56de08a7-a295-4067-b302-1b0f6cc2e970",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Loading pre-calculated model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93fad79-4452-4329-9c6c-bca5cccf31fa",
   "metadata": {},
   "source": [
    "The suite below is designed to be run after a model was trained, and so requires model predictions and can be supplied via the relevant arguments in the ``run`` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756f73ce-01c5-4e82-b6b3-376073197af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds, test_preds = tweet_emotion.load_precalculated_predictions(pred_format='predictions', as_train_test=True)\n",
    "\n",
    "train_probas, test_probas = tweet_emotion.load_precalculated_predictions(pred_format='probabilities', as_train_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058775ad-0520-4eb6-a649-b890cf52fc4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepchecks.nlp.suites import model_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c76de48-b2ad-4df2-90ef-b7bd42d2739a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result = model_evaluation().run(train, test, train_predictions=train_preds, test_predictions=test_preds, \n",
    "                                train_probabilities=train_probas, test_probabilities=test_probas, model_classes=model_classes)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f784020-a3db-46ce-bcd1-21b8a4aea1cf",
   "metadata": {},
   "source": [
    "OK! We have many important issues being surfaced by this suite. Let's dive into the individual checks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5cd3b-4ebc-4ac8-a64e-0ee732014a07",
   "metadata": {},
   "source": [
    "### Model Eval #1: Train Test Performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744861c-d777-43e0-82fa-ce634bdbdbac",
   "metadata": {},
   "source": [
    "On the most superficial level, we can immediately see (in the \"Didn't Pass\" tab) that there has been significant degradation in the Recall on class \"optimism\". This follows from the severe label drift we saw after running the previous suite."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15817ad7-c533-4ded-b4bc-d192d5dab0d5",
   "metadata": {},
   "source": [
    "### Model Eval #2: Segment Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8857b45-d1d7-4689-b725-96af7fb37c84",
   "metadata": {},
   "source": [
    "The two segment performance checks - Property Segment Performance and Meadata Segment Performance, use the metadata columns of user related information OR our calculated properties to try and **automatically** detect significant data segments on which our model performs badly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1e33b-f2ac-4c4a-b904-64b66e5fc5a6",
   "metadata": {},
   "source": [
    "In this case we can see that both checks have found issues in the test dataset:\n",
    "1. The Property Segment Performance check has found that we're getting very poor results on low toxicity samples. That probably means that our model is using the toxicity of the text to infer the \"anger\" label, and is having a harder problem with other, more benign text samples.\n",
    "2. The Metadata Segment Performance check has found that we have predicting correct results on new users from the Americas. That's 5% of our dataset so we better investigate that further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628edf9-6eb2-442d-a717-2b2320843401",
   "metadata": {},
   "source": [
    "### Model Eval #3: Prediction Drift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a83f1-7c37-48a9-997b-af8d53068ad0",
   "metadata": {},
   "source": [
    "We note that the Prediction Drift (here in the \"Passed\" tab) shows no issue. Given that we already know that there is significant Label Drift, this means we have Concept Drift - the labels corresponding to our samples have changed, while the model continues to predict the same labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee60f05-0c47-46fb-a8d1-c04da4c54072",
   "metadata": {},
   "source": [
    "## Running all check in one suite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b400d2-5425-4363-9ba4-ebb6f9b89c8d",
   "metadata": {},
   "source": [
    "If you have a model that you already developed and you want to test all available checks at once, you can run the Full Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af937c43-c6a0-42f5-973b-c489fc5e68b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from deepchecks.nlp.suites import full_suite\n",
    "# suite = full_suite()\n",
    "# result = suite.run(train, test, train_predictions=train_preds, test_predictions=test_preds, \n",
    "#                    train_probabilities=train_probas, test_probabilities=test_probas, model_classes=model_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a30e13-f7df-435b-9f12-012867ca57b8",
   "metadata": {},
   "source": [
    "# Running Individual Checks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739f4681-4085-4da8-8d44-87955ecd55cd",
   "metadata": {},
   "source": [
    "Checks can also be run individually. In this section, we'll show two of the more interesting checks and how you can run them stand-alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397c969-49db-4083-9649-8c141833f942",
   "metadata": {},
   "source": [
    "## Embeddings Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1addb64e-122f-4d9d-894e-4855df178268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepchecks.nlp.datasets.classification.tweet_emotion import load_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9349fd25-4a2a-4f38-b48a-70248727f21e",
   "metadata": {},
   "source": [
    "In order to run the embedding drift check you must have text embeddings loaded to both datasets. In this example, we have the embeddings already pre-calculated: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab4d3c-d7c0-4318-9ef0-caab918ffc44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_embeddings, test_embeddings = load_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf9c7f9-2bc0-4ce6-a86a-30412d29d3f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.set_embeddings(train_embeddings)\n",
    "test.set_embeddings(test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd1cb48-7f00-49cc-a368-9e571cc099f6",
   "metadata": {},
   "source": [
    "You can also calculate the embeddings using deepchecks, either using an open-source sentence-transformer or using Open AI's embedding API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237964a-2cbc-4072-962a-b2e68859de63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train.calculate_default_embeddings()\n",
    "# test.calculate_default_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a9ad29-722f-4490-96c4-8067f6568f28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepchecks.nlp.checks import TextEmbeddingsDrift\n",
    "\n",
    "check = TextEmbeddingsDrift()\n",
    "res = check.run(train, test)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085902e1-6ff3-4b68-8467-6d7cb77d10f3",
   "metadata": {},
   "source": [
    "Here we can see some distinct segments that distinctly contain more samples from train or more sample for test. For example, if we look at the cluster on the top left corner we see it's full of inspirational quotes and saying, and belongs mostly to the test dataset. That is the source of the drastic increase in optimistic labels!\n",
    "\n",
    "There are some other note-worthy segments, such as the \"tail\" segment in the middle left that contains tweets about a terror attack in Bangladesh (and belongs solely to the test data), or a cluster on the bottom right that discusses a sports event that probably happened strictly in the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908ab22-a530-4ed9-8bb5-5e5591a3d733",
   "metadata": {},
   "source": [
    "## Under Annotated Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6094a20d-0b1c-4852-ba2d-9c2463a59481",
   "metadata": {},
   "source": [
    "Another note-worth segment is the Under Annotated Segment check, which explores our data and automatically identifies segments where the data is under-annotated - meaning that the ratio of missing labels is higher. To this check we'll also add a condition that will alert us in case that a significant under-annotated segment is found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716e8a7-fd97-4a1d-a25d-fea5e827f228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from deepchecks.nlp.checks import UnderAnnotatedPropertySegments\n",
    "check = UnderAnnotatedPropertySegments(segment_minimum_size_ratio=0.1).add_condition_segments_relative_performance_greater_than()\n",
    "check.run(test_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3fbc2d-43eb-4a4f-8cbe-b15d6c86f398",
   "metadata": {},
   "source": [
    "For example here the check detected that we have a lot of lacking annotations for samples that are informal and not very fluent. May it be the case that our annotators have a problem annotating these samples and prefer not to deal with them? If these samples are important for use, we may have to put special focus on annotating this segment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
